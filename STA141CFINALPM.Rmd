---
title: "STA141C Final Project"
author: "Your Name"
date: "2024-05-25"
output: html_document
---

# Introduction

The goal is to predict the severity of obesity problems using various machine learning techniques.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load Libraries
library(tidyverse)
library(dplyr)
library(readr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(glmnet)
library(pROC)
library(ggplot2)
library(corrplot)
library(car)
library(e1071) 
library(reshape)
library(broom)
library(knitr)
library(kableExtra)
```



```{r load-data}
# Read the data
data <- read.csv("US Project Data.csv")

# Select the needed columns
data <- data %>%
  select(MedianIncE, P_HS, P_FastFood, P_Doctor, Obesity_Prob)

# Recode the response variable as numeric
data$ObesityProb_numeric <- ifelse(data$Obesity_Prob == "Higher Severity", 1, 0)

# Convert the response variable to a factor with custom labels
data$ObesityProb_numeric <- factor(data$ObesityProb_numeric, levels = c(0, 1), labels = c("Lower Severity", "Higher Severity"))



```


```{r split-data}
# Split the data into training (60%) and testing (40%) sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$ObesityProb_numeric, p = 0.6, list = FALSE)

trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Separate predictors (X) and target (Y)
X_train <- trainData %>% select(MedianIncE, P_HS, P_FastFood, P_Doctor)
Y_train <- trainData$ObesityProb_numeric

X_test <- testData %>% select(MedianIncE, P_HS, P_FastFood, P_Doctor)
Y_test <- testData$ObesityProb_numeric

# Convert to matrix for glmnet using model.matrix
X_train_matrix <- model.matrix(ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, trainData)[, -1]
X_test_matrix <- model.matrix(ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, testData)[, -1]

# Convert Y_train and Y_test to numeric
Y_train_numeric <- ifelse(Y_train == "Higher Severity", 1, 0)
Y_test_numeric <- ifelse(Y_test == "Higher Severity", 1, 0)

# Print the sizes of the training and testing sets
#print(paste("Training set size:", nrow(trainData)))
#print(paste("Testing set size:", nrow(testData)))

# Create summary tables for class distribution
#train_summary <- trainData %>%
#  group_by(ObesityProb_numeric) %>%
#  summarise(count = n(), proportion = n() / nrow(trainData))

#test_summary <- testData %>%
#  group_by(ObesityProb_numeric) %>%
 # summarise(count = n(), proportion = n() / nrow(testData))

# Print the summary tables
#print("Training set class distribution:")
#print(train_summary)

#print("Testing set class distribution:")
#print(test_summary)
# Check for missing values in the training data
# Check for missing values in the training data


```

#Section 3.1 (Logistic Regression)
```{r}
# Define the function to fit logistic regression and calculate performance metrics
logReg_feat <- function(feature_set, data_train, data_test, y_train, y_test) {
  formula <- as.formula(paste("ObesityProb_numeric ~", paste(feature_set, collapse = " + ")))
  model <- glm(formula, data = data_train, family = binomial)
  
  # Predict on the test set
  predictions <- predict(model, newdata = data_test, type = "response")
  predictions_binary <- ifelse(predictions >= 0.5, "Higher Severity", "Lower Severity")
  
  accuracy <- mean(predictions_binary == y_test)
  aic <- AIC(model)
  bic <- BIC(model)
  
  return(list(features = feature_set, model = model, AIC = aic, BIC = bic, Accuracy = accuracy))
}

# Function to evaluate all combinations of features
getBest <- function(k, feature_names, data_train, data_test, y_train, y_test) {
  results <- list()
  
  combos <- combn(feature_names, k, simplify = FALSE)
  for (combo in combos) {
    results[[paste(combo, collapse = "+")]] <- logReg_feat(combo, data_train, data_test, y_train, y_test)
  }
  
  models <- do.call(rbind, lapply(results, function(x) data.frame(AIC = x$AIC, BIC = x$BIC, Accuracy = x$Accuracy, features = paste(x$features, collapse = "+"))))
  best_model <- models[which.min(models$AIC),]
  
  return(best_model)
}

# Find the best model for each number of features
feature_names <- colnames(X_train)
models_best <- data.frame(AIC = numeric(), BIC = numeric(), Accuracy = numeric(), features = character())

for (i in 1:length(feature_names)) {
  best_model <- getBest(i, feature_names, trainData, testData, Y_train, Y_test)
  models_best <- rbind(models_best, best_model)
}

# Display the best models by feature count
print(models_best)
```

```{r nicer-table, echo=TRUE, message=FALSE, warning=FALSE}


# Create the dataframe with your results
models_best <- data.frame(
  AIC = c(1774.612, 1666.022, 1610.928, 1563.279),
  BIC = c(1785.357, 1682.138, 1632.417, 1590.139),
  Accuracy = c(0.7101039, 0.7186025, 0.7497639, 0.7620397),
  features = c("MedianIncE", "MedianIncE+P_HS", "MedianIncE+P_HS+P_Doctor", "MedianIncE+P_HS+P_FastFood+P_Doctor")
)

# Format the table
kable(models_best, caption = "Best Logistic Regression Models by Number of Features") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(4, width = "20em") %>%
  row_spec(0, bold = TRUE, background = "#D3D3D3")
```




```{r}
# Final Model with all predictors
final_model <- glm(ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, data = trainData, family = binomial)

logistic_probabilities <- predict(final_model, newdata = testData, type = "response")
logistic_predictions <- ifelse(logistic_probabilities >= 0.5, "Higher Severity", "Lower Severity")
logistic_cm <- confusionMatrix(as.factor(logistic_predictions), testData$ObesityProb_numeric)
print(logistic_cm)
```

```{r}
# Check for multicollinearity
vif(final_model)
```

```{r}
#Diagnostic Plots
par(mfrow = c(2, 2))
plot(final_model)
```

```{r}
#validation with K-fold cross-validation
# Set up training control for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Perform 10-fold cross-validation
set.seed(123)  # For reproducibility
cv_model <- train(ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, 
                  data = trainData, 
                  method = "glm", 
                  family = "binomial", 
                  trControl = train_control)

# Print cross-validation results
print(cv_model)

```
```{r}
# Predict probabilities on the test set
probabilities <- predict(final_model, newdata = testData, type = "response")

# Compute ROC curve
roc_curve <- roc(testData$ObesityProb_numeric, probabilities)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve")
```

```{r}
# Predict binary outcomes
predictions <- ifelse(probabilities >= 0.5, "Higher Severity", "Lower Severity")

# Confusion matrix
confusionMatrix(as.factor(predictions), testData$ObesityProb_numeric)
```



```{r}
# Fit Lasso regression model with cross-validation
set.seed(123)
lasso_model <- cv.glmnet(X_train_matrix, Y_train_numeric, alpha = 1, family = "binomial")

# Predict probabilities on the test set
lasso_probabilities <- predict(lasso_model, newx = X_test_matrix, s = lasso_model$lambda.min, type = "response")
lasso_predictions <- ifelse(lasso_probabilities >= 0.5, "Higher Severity", "Lower Severity")

# Convert predictions and true values to factors
lasso_predictions <- factor(lasso_predictions, levels = levels(testData$ObesityProb_numeric))
Y_test_factor <- factor(testData$ObesityProb_numeric, levels = levels(testData$ObesityProb_numeric))

# Calculate the confusion matrix
lasso_cm <- confusionMatrix(lasso_predictions, Y_test_factor)

# Manually calculate AIC and BIC
log_likelihood <- sum(ifelse(Y_test_factor == "Higher Severity", log(lasso_probabilities), log(1 - lasso_probabilities)))
n <- length(Y_test_factor)
p <- sum(coef(lasso_model, s = "lambda.min") != 0) - 1  # number of non-zero predictors excluding the intercept

aic_lasso <- -2 * log_likelihood + 2 * p
bic_lasso <- -2 * log_likelihood + log(n) * p

# Display results
print(lasso_cm)
print(paste("AIC:", round(aic_lasso, 2)))
print(paste("BIC:", round(bic_lasso, 2)))

# Display coefficients
lasso_coef <- coef(lasso_model, s = lasso_model$lambda.min)
print(lasso_coef)
```

```{r}
# Fit Ridge regression model with cross-validation
set.seed(123)
ridge_model <- cv.glmnet(X_train_matrix, Y_train_numeric, alpha = 0, family = "binomial")
ridge_probabilities <- predict(ridge_model, newx = X_test_matrix, s = ridge_model$lambda.min, type = "response")
ridge_predictions <- ifelse(ridge_probabilities >= 0.5, "Higher Severity", "Lower Severity")

# Convert predictions and true values to factors
ridge_predictions <- factor(ridge_predictions, levels = levels(testData$ObesityProb_numeric))
Y_test_factor <- factor(testData$ObesityProb_numeric, levels = levels(testData$ObesityProb_numeric))

# Calculate the confusion matrix
ridge_cm <- confusionMatrix(ridge_predictions, Y_test_factor)

# Manually calculate AIC and BIC
log_likelihood_ridge <- sum(ifelse(Y_test_factor == "Higher Severity", log(ridge_probabilities), log(1 - ridge_probabilities)))
n_ridge <- length(Y_test_factor)
p_ridge <- sum(coef(ridge_model, s = "lambda.min") != 0) - 1  # number of non-zero predictors excluding the intercept

aic_ridge <- -2 * log_likelihood_ridge + 2 * p_ridge
bic_ridge <- -2 * log_likelihood_ridge + log(n_ridge) * p_ridge

# Display results
print(ridge_cm)
print(paste("AIC:", round(aic_ridge, 2)))
print(paste("BIC:", round(bic_ridge, 2)))

# Display coefficients
ridge_coef <- coef(ridge_model, s = ridge_model$lambda.min)
print(ridge_coef)
```
```{r}
# Create a comparison table for performance metrics
model_comparison <- data.frame(
  Metric = c("Accuracy", "Sensitivity", "Specificity", "Precision (PPV)", "NPV", "AIC", "BIC"),
  Lasso = c(
    lasso_cm$overall['Accuracy'],
    lasso_cm$byClass['Sensitivity'],
    lasso_cm$byClass['Specificity'],
    lasso_cm$byClass['Pos Pred Value'],
    lasso_cm$byClass['Neg Pred Value'],
    round(aic_lasso, 2),
    round(bic_lasso, 2)
  ),
  Ridge = c(
    ridge_cm$overall['Accuracy'],
    ridge_cm$byClass['Sensitivity'],
    ridge_cm$byClass['Specificity'],
    ridge_cm$byClass['Pos Pred Value'],
    ridge_cm$byClass['Neg Pred Value'],
    round(aic_ridge, 2),
    round(bic_ridge, 2)
  )
)

# Display the comparison table
library(knitr)
library(kableExtra)
kable(model_comparison, caption = "Comparison of Lasso and Ridge Regression Models") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE, background = "#D3D3D3")
```

```{r roc-curve-comparison, echo=TRUE, message=FALSE, warning=FALSE}

# Compute ROC curve for Lasso
lasso_probabilities <- predict(lasso_model, newx = X_test_matrix, s = lasso_model$lambda.min, type = "response")
lasso_roc_curve <- roc(testData$ObesityProb_numeric, lasso_probabilities)

# Compute ROC curve for Ridge
ridge_probabilities <- predict(ridge_model, newx = X_test_matrix, s = ridge_model$lambda.min, type = "response")
ridge_roc_curve <- roc(testData$ObesityProb_numeric, ridge_probabilities)

# Plot ROC curves
plot(lasso_roc_curve, col = "blue", main = "ROC Curve Comparison", print.auc = TRUE)
lines(ridge_roc_curve, col = "red")
legend("bottomright", legend = c("Lasso", "Ridge"), col = c("blue", "red"), lty = 1)
```


```{r decision-tree, echo=TRUE, message=FALSE, warning=FALSE}
# Fit a decision tree model
tree_model <- rpart(ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, data = trainData, method = "class")

# Predict on the test set
tree_predictions <- predict(tree_model, newdata = testData, type = "class")

# Confusion matrix
tree_cm <- confusionMatrix(tree_predictions, testData$ObesityProb_numeric)

# Print confusion matrix
print(tree_cm)

rpart.plot(tree_model, main = "Decision Tree")
```

```{r}
# Fit a random forest model
set.seed(123)
rf_model <- randomForest(ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, data = trainData, ntree = 100)

# Predict on the test set
rf_predictions <- predict(rf_model, newdata = testData)

# Confusion matrix
rf_cm <- confusionMatrix(rf_predictions, testData$ObesityProb_numeric)

# Print confusion matrix
print(rf_cm)

# Plot variable importance for Random Forest
importance_rf <- importance(rf_model)
varImpPlot(rf_model, main = "Feature Importance (Random Forest)")
```






---
title: "STA141C Final Project"
author: "Your Name"
date: "2024-05-25"
output: html_document
---

# Introduction

The goal is to predict the severity of obesity problems using various machine learning techniques.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load Libraries
library(tidyverse)
library(dplyr)
library(readr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(glmnet)
library(pROC)
library(ggplot2)
library(corrplot)
library(car)
library(e1071) 
library(reshape)
library(broom)

```



```{r load-data}
# Read the data
data <- read.csv("US Project Data.csv")

# Select the needed columns
data <- data %>%
  select(MedianIncE, P_HS, P_FastFood, P_Doctor, Obesity_Prob)

# Recode the response variable as numeric
data$ObesityProb_numeric <- ifelse(data$Obesity_Prob == "Higher Severity", 1, 0)

# Convert the response variable to a factor with valid R names
data$ObesityProb_numeric <- factor(data$ObesityProb_numeric, levels = c(0, 1), labels = c("Class0", "Class1"))

# View the first few rows of the modified data
#print(head(data))


#df_summary = data %>%
#  group_by(Obesity_Prob) %>%
#  summarise(count = n(),
#            proportion = n() / nrow(data))
#df_summary


#this is a 49:51 ratio, which is relatively balanced. 

# Recode as numeric
#data = data %>%
#  mutate(
#    ObesityProb_numeric = case_when(
#      Obesity_Prob == "Higher Severity" ~ 1,
#      Obesity_Prob == "Lower Severity" ~ 0
 



```


```{r split-data}
# Split the data into training (60%) and testing (40%) sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$ObesityProb_numeric, p = 0.6, list = FALSE)

trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Separate predictors (X) and target (Y)
X_train <- trainData %>% select(MedianIncE, P_HS, P_FastFood, P_Doctor)
Y_train <- trainData$ObesityProb_numeric

X_test <- testData %>% select(MedianIncE, P_HS, P_FastFood, P_Doctor)
Y_test <- testData$ObesityProb_numeric

# Ensure the factor levels of Y_train are valid R variable names
levels(Y_train) <- make.names(levels(Y_train))


# Print the sizes of the training and testing sets
#print(paste("Training set size:", nrow(trainData)))
#print(paste("Testing set size:", nrow(testData)))

# Create summary tables for class distribution
#train_summary <- trainData %>%
#  group_by(ObesityProb_numeric) %>%
#  summarise(count = n(), proportion = n() / nrow(trainData))

#test_summary <- testData %>%
#  group_by(ObesityProb_numeric) %>%
 # summarise(count = n(), proportion = n() / nrow(testData))

# Print the summary tables
#print("Training set class distribution:")
#print(train_summary)

#print("Testing set class distribution:")
#print(test_summary)

```


```{r}
# Fit the logistic regression model with all predictors and their interactions
model <- glm(Y_train ~ (MedianIncE + P_HS + P_FastFood + P_Doctor)^2, 
             data = cbind(X_train, Y_train), 
             family = binomial())

# Summary of the model
summary(model)

```


```{r}
# Perform stepwise selection to find the best model
#best_model <- step(initial_model, direction = "both")

# Summary of the best model
#summary(best_model)
```

```{r}
# Generate predictions on the testing data using the model
testData$logistic_predictions <- predict(model, newdata = X_test, type = "response")

# Evaluate the model performance on the testing set
# Convert predictions to binary outcomes using a threshold of 0.5
testData$predicted_class <- ifelse(testData$logistic_predictions > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix <- table(Predicted = testData$predicted_class, Actual = Y_test)

# Print the confusion matrix
print(confusion_matrix)
```

```{r}
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy: ", accuracy))

# Calculate feature importance
importance <- broom::tidy(model) %>%
  mutate(importance = abs(estimate)) %>%
  arrange(desc(importance))

# Print feature importance
print(importance)

# Plot feature importance
importance %>%
  ggplot(aes(x = reorder(term, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance", x = "Features", y = "Importance") +
  theme_minimal()
```

```{r}
# Fit the classification tree with tuning parameters
tree_model <- rpart(Y_train ~ MedianIncE + P_HS + P_FastFood + P_Doctor, 
                    data = cbind(X_train, Y_train), method = "class",
                    control = rpart.control(minsplit = 20, cp = 0.01))

# Print and plot the tree model with rpart.plot for better visualization
print(tree_model)
rpart.plot(tree_model)

# Generate predictions on the testing data using the tree model
testData$tree_predictions <- predict(tree_model, newdata = X_test, type = "class")

# Create a confusion matrix for the classification tree
confusion_matrix_tree <- table(Predicted = testData$tree_predictions, Actual = Y_test)

# Print the confusion matrix for the classification tree
print(confusion_matrix_tree)

# Calculate accuracy for the classification tree
accuracy_tree <- sum(diag(confusion_matrix_tree)) / sum(confusion_matrix_tree)
print(paste("Classification Tree Accuracy: ", accuracy_tree))
```
```{r}
# Define the parameter grid for hyperparameter tuning
tune_grid <- expand.grid(mtry = c(1, 2, 3, 4))

# Perform hyperparameter tuning using caret
control <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)

tuned_rf <- train(
  x = X_train,
  y = Y_train,
  method = "rf",
  metric = "ROC",
  tuneGrid = tune_grid,
  trControl = control
)

# Print the best parameters
print(tuned_rf$bestTune)
```



```{r}
# Fit the random forest model with the best parameters
best_rf_model <- randomForest(
  ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, 
  data = trainData, 
  ntree = 500, 
  mtry = tuned_rf$bestTune$mtry, 
  importance = TRUE
)

# Print the random forest model
print(best_rf_model)
```
```{r}
# Generate predictions on the testing data using the random forest model
testData$rf_predictions <- predict(best_rf_model, newdata = X_test, type = "prob")[,2]

# Calculate ROC-AUC
roc_obj <- roc(Y_test, testData$rf_predictions)
roc_auc <- auc(roc_obj)
print(paste("Random Forest ROC-AUC: ", roc_auc))

# Plot the ROC curve
plot(roc_obj, main = paste("ROC Curve - AUC:", round(roc_auc, 3)))

# Create a confusion matrix for the random forest
testData$rf_pred_class <- ifelse(testData$rf_predictions > 0.5, "Class1", "Class0")
confusion_matrix_rf <- table(Predicted = testData$rf_pred_class, Actual = Y_test)

# Print the confusion matrix for the random forest
print(confusion_matrix_rf)

# Calculate accuracy for the random forest
accuracy_rf <- sum(diag(confusion_matrix_rf)) / sum(confusion_matrix_rf)
print(paste("Random Forest Accuracy: ", accuracy_rf))
```

```{r}
# Plot variable importance
importance_rf <- importance(best_rf_model)
varImpPlot(best_rf_model)
```


---
title: "STA141C Final Project"
author: "Your Name"
date: "2024-05-25"
output: html_document
---

# Introduction

The goal is to predict the severity of obesity problems using various machine learning techniques.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load Libraries
library(tidyverse)
library(dplyr)
library(readr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(glmnet)
library(pROC)
library(ggplot2)
library(corrplot)
library(car)
library(e1071) 
library(reshape)
library(broom)

```



```{r load-data}
# Read the data
data <- read.csv("US Project Data.csv")

# Select the needed columns
data <- data %>%
  select(MedianIncE, P_HS, P_FastFood, P_Doctor, Obesity_Prob)

# Recode the response variable as numeric
data$ObesityProb_numeric <- ifelse(data$Obesity_Prob == "Higher Severity", 1, 0)

# Convert the response variable to a factor with custom labels
data$ObesityProb_numeric <- factor(data$ObesityProb_numeric, levels = c(0, 1), labels = c("Lower Severity", "Higher Severity"))

# View the first few rows of the modified data
#print(head(data))


#df_summary = data %>%
#  group_by(Obesity_Prob) %>%
#  summarise(count = n(),
#            proportion = n() / nrow(data))
#df_summary


#this is a 49:51 ratio, which is relatively balanced. 

# Recode as numeric
#data = data %>%
#  mutate(
#    ObesityProb_numeric = case_when(
#      Obesity_Prob == "Higher Severity" ~ 1,
#      Obesity_Prob == "Lower Severity" ~ 0
 



```


```{r split-data}
# Split the data into training (60%) and testing (40%) sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$ObesityProb_numeric, p = 0.6, list = FALSE)

trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Separate predictors (X) and target (Y)
X_train <- trainData %>% select(MedianIncE, P_HS, P_FastFood, P_Doctor)
Y_train <- trainData$ObesityProb_numeric

X_test <- testData %>% select(MedianIncE, P_HS, P_FastFood, P_Doctor)
Y_test <- testData$ObesityProb_numeric

# Convert to matrix for glmnet
X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)

# Convert Y_train and Y_test to numeric
Y_train_numeric <- ifelse(Y_train == "Higher Severity", 1, 0)
Y_test_numeric <- ifelse(Y_test == "Higher Severity", 1, 0)

# Print the sizes of the training and testing sets
#print(paste("Training set size:", nrow(trainData)))
#print(paste("Testing set size:", nrow(testData)))

# Create summary tables for class distribution
#train_summary <- trainData %>%
#  group_by(ObesityProb_numeric) %>%
#  summarise(count = n(), proportion = n() / nrow(trainData))

#test_summary <- testData %>%
#  group_by(ObesityProb_numeric) %>%
 # summarise(count = n(), proportion = n() / nrow(testData))

# Print the summary tables
#print("Training set class distribution:")
#print(train_summary)

#print("Testing set class distribution:")
#print(test_summary)
# Check for missing values in the training data
# Check for missing values in the training data


```


```{r}
# Fit the logistic regression model with all predictors and their interactions
model <- glm(Y_train ~ (MedianIncE + P_HS + P_FastFood + P_Doctor)^2, 
             data = cbind(X_train, Y_train), 
             family = binomial())

# Summary of the model
summary(model)
```
```{r}

# Generate predictions on the testing data using the model
testData$logistic_predictions <- predict(model, newdata = X_test, type = "response")

# Evaluate the model performance on the testing set
# Convert predictions to binary outcomes using a threshold of 0.5
testData$predicted_class <- ifelse(testData$logistic_predictions > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix <- table(Predicted = testData$predicted_class, Actual = Y_test)

# Print the confusion matrix
print(confusion_matrix)
```

```{r}
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy: ", accuracy))

# Calculate feature importance
importance <- broom::tidy(model) %>%
  mutate(importance = abs(estimate)) %>%
  arrange(desc(importance))

# Print feature importance
print(importance)

# Plot feature importance
importance %>%
  ggplot(aes(x = reorder(term, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance", x = "Features", y = "Importance") +
  theme_minimal()
```


```{r}
# Fit the Lasso model
lasso_model <- cv.glmnet(X_train_matrix, Y_train_numeric, alpha = 1, family = "binomial")

# Get the best lambda value
best_lambda_lasso <- lasso_model$lambda.min

# Predict on test data
lasso_predictions <- predict(lasso_model, newx = X_test_matrix, s = best_lambda_lasso, type = "response")
lasso_pred_class <- ifelse(lasso_predictions > 0.5, 1, 0)

# Ensure factor levels are the same for comparison
lasso_pred_class <- factor(lasso_pred_class, levels = c(0, 1), labels = c("Lower Severity", "Higher Severity"))
Y_test <- factor(Y_test, levels = c("Lower Severity", "Higher Severity"))

# Evaluate model
confusion_matrix_lasso <- table(Predicted = lasso_pred_class, Actual = Y_test)
roc_obj_lasso <- roc(as.numeric(Y_test_numeric), lasso_predictions)
roc_auc_lasso <- auc(roc_obj_lasso)

precision_lasso <- posPredValue(lasso_pred_class, Y_test)
recall_lasso <- sensitivity(lasso_pred_class, Y_test)
f1_score_lasso <- (2 * precision_lasso * recall_lasso) / (precision_lasso + recall_lasso)

print(paste("Lasso ROC-AUC: ", roc_auc_lasso))
print(paste("Lasso Precision: ", precision_lasso))
print(paste("Lasso Recall: ", recall_lasso))
print(paste("Lasso F1-Score: ", f1_score_lasso))
print(paste("Lasso Accuracy: ", sum(diag(confusion_matrix_lasso)) / sum(confusion_matrix_lasso)))
```

```{r}
# Fit the Ridge model
ridge_model <- cv.glmnet(X_train_matrix, Y_train_numeric, alpha = 0, family = "binomial")

# Get the best lambda value
best_lambda_ridge <- ridge_model$lambda.min

# Predict on test data
ridge_predictions <- predict(ridge_model, newx = X_test_matrix, s = best_lambda_ridge, type = "response")
ridge_pred_class <- ifelse(ridge_predictions > 0.5, 1, 0)

# Ensure factor levels are the same for comparison
ridge_pred_class <- factor(ridge_pred_class, levels = c(0, 1), labels = c("Lower Severity", "Higher Severity"))
Y_test <- factor(Y_test, levels = c("Lower Severity", "Higher Severity"))

# Evaluate model
confusion_matrix_ridge <- table(Predicted = ridge_pred_class, Actual = Y_test)
roc_obj_ridge <- roc(as.numeric(Y_test_numeric), ridge_predictions)
roc_auc_ridge <- auc(roc_obj_ridge)

precision_ridge <- posPredValue(ridge_pred_class, Y_test)
recall_ridge <- sensitivity(ridge_pred_class, Y_test)
f1_score_ridge <- (2 * precision_ridge * recall_ridge) / (precision_ridge + recall_ridge)

print(paste("Ridge ROC-AUC: ", roc_auc_ridge))
print(paste("Ridge Precision: ", precision_ridge))
print(paste("Ridge Recall: ", recall_ridge))
print(paste("Ridge F1-Score: ", f1_score_ridge))
print(paste("Ridge Accuracy: ", sum(diag(confusion_matrix_ridge)) / sum(confusion_matrix_ridge)))
```




```{r}
# Fit the classification tree with tuning parameters
tree_model <- rpart(Y_train ~ MedianIncE + P_HS + P_FastFood + P_Doctor, 
                    data = cbind(X_train, Y_train), method = "class",
                    control = rpart.control(minsplit = 20, cp = 0.01))

# Print and plot the tree model with rpart.plot for better visualization
print(tree_model)
rpart.plot(tree_model)

# Generate predictions on the testing data using the tree model
testData$tree_predictions <- predict(tree_model, newdata = X_test, type = "class")

# Create a confusion matrix for the classification tree
confusion_matrix_tree <- table(Predicted = testData$tree_predictions, Actual = Y_test)

# Print the confusion matrix for the classification tree
print(confusion_matrix_tree)

# Calculate accuracy for the classification tree
accuracy_tree <- sum(diag(confusion_matrix_tree)) / sum(confusion_matrix_tree)
print(paste("Classification Tree Accuracy: ", accuracy_tree))
```
```{r}
Y_train_valid <- factor(Y_train, levels = c("Lower Severity", "Higher Severity"), labels = c("Class0", "Class1"))

# Define the parameter grid for hyperparameter tuning
tune_grid <- expand.grid(mtry = c(1, 2, 3, 4))

# Perform hyperparameter tuning using caret
control <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)

tuned_rf <- train(
  x = X_train,
  y = Y_train_valid,
  method = "rf",
  metric = "ROC",
  tuneGrid = tune_grid,
  trControl = control
)

# Print the best parameters
print(tuned_rf$bestTune)
```



```{r}
# Fit the random forest model with the best parameters
best_rf_model <- randomForest(
  ObesityProb_numeric ~ MedianIncE + P_HS + P_FastFood + P_Doctor, 
  data = trainData, 
  ntree = 500, 
  mtry = tuned_rf$bestTune$mtry, 
  importance = TRUE
)

# Print the random forest model
print(best_rf_model)
```
```{r}
# Generate predictions on the testing data using the random forest model
testData$rf_predictions <- predict(best_rf_model, newdata = X_test, type = "prob")[,2]

# Calculate ROC-AUC
roc_obj <- roc(Y_test, testData$rf_predictions)
roc_auc <- auc(roc_obj)
print(paste("Random Forest ROC-AUC: ", roc_auc))

# Plot the ROC curve
plot(roc_obj, main = paste("ROC Curve - AUC:", round(roc_auc, 3)))

# Create a confusion matrix for the random forest
testData$rf_pred_class <- ifelse(testData$rf_predictions > 0.5, "Class1", "Class0")
confusion_matrix_rf <- table(Predicted = testData$rf_pred_class, Actual = Y_test)

# Print the confusion matrix for the random forest
print(confusion_matrix_rf)

# Calculate accuracy for the random forest
accuracy_rf <- sum(diag(confusion_matrix_rf)) / sum(confusion_matrix_rf)
print(paste("Random Forest Accuracy: ", accuracy_rf))
```

```{r}
# Plot variable importance
importance_rf <- importance(best_rf_model)
varImpPlot(best_rf_model)
```

